{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import download_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import download_dataset  # dowloading from google drive\n",
    "import ocr_input         # deals with reading dataset and xml parsing\n",
    "\n",
    "\n",
    "for elt in [\"label\", \"image\", \"ocr\", \"dataset_assignment\"]:\n",
    "    download_dataset.download_and_extract(elt)\n",
    "\n",
    "dataset_path = \"dataset\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unzipping ./tmp/image.zip to dataset/image…\n",
      "Unzipping ./tmp/ocr.zip to dataset/ocr…\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Read train / test split and labels\n",
    "raw_class_indices = ['1', '2', '3', '4', '11']\n",
    "labels = {}\n",
    "with open(os.path.join(dataset_path, \"label.txt\"), \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        file_id, lbl = line.strip().split(\",\")\n",
    "        labels[file_id] = lbl\n",
    "\n",
    "dataset = defaultdict(list)\n",
    "with open(os.path.join(dataset_path, \"dataset_assignment.txt\"), \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split('\\n')[0]\n",
    "        file_id, assignment = line.split(',')\n",
    "        img_path = os.path.join(dataset_path, \"image_png\", f\"{file_id}.png\")\n",
    "        ocr_path = os.path.join(dataset_path, \"ocr\", f\"{file_id}.xml\")\n",
    "        \n",
    "        dataset[f\"{assignment}_img\"].append(img_path)\n",
    "        dataset[f\"{assignment}_ocr\"].append(ocr_path)\n",
    "        dataset[f\"{assignment}_lbl\"].append(labels[file_id])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "dataset[\"test_ocr\"][0], dataset[\"test_lbl\"][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('dataset/ocr/aaa11d00.xml', '3')"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "list_train_ds = tf.data.Dataset.from_tensor_slices(dataset['training_img'])\n",
    "list_train_ds = list_train_ds.shuffle(100000)\n",
    "list_test_ds = tf.data.Dataset.from_tensor_slices(dataset['test_img'])\n",
    "list_test_ds = list_test_ds.shuffle(100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'dataset/image_png/cul15a00.png'\n",
      "b'dataset/image_png/cpa65a00.png'\n",
      "b'dataset/image_png/art18c00.png'\n",
      "b'dataset/image_png/aso88c00.png'\n",
      "b'dataset/image_png/cfl06e00.png'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('.venv')"
  },
  "interpreter": {
   "hash": "b9d6bdd8f7dde5cd7f6196e727254796a610d31de0bf8d3fdd813982d902c478"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "skeleton_ocr.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clemsage/NeuralDocumentClassification/blob/master/skeleton_ocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56cA2WtUQZ-I",
        "colab_type": "text"
      },
      "source": [
        "# Training a classifier on OCR text input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAd75X5JT4vI",
        "colab_type": "text"
      },
      "source": [
        "## Imports & Cloning repository\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uefjUmcMRnLT",
        "colab_type": "text"
      },
      "source": [
        "### Import Tensorflow v2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVIlDgNHW62j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0\n",
        "!pip install tensorflow-gpu==2.0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak-qq01rXLxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check your devices, if it fails change your execution context to GPU\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Ki-ZY9RyXl",
        "colab_type": "text"
      },
      "source": [
        "### Usefull imports and clone repo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vFz745yYfbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "print(sys.version)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL_khQsbYY7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the git repository\n",
        "\n",
        "if not os.path.exists('NeuralDocumentClassification'):\n",
        "  !git clone https://github.com/clemsage/NeuralDocumentClassification.git\n",
        "else:\n",
        "  !git -C NeuralDocumentClassification pull\n",
        "sys.path.append('NeuralDocumentClassification')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp4eXRIKW62o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lot of usefull imports\n",
        "\n",
        "# All of them are already installed on the colab session\n",
        "\n",
        "\n",
        "# STD imports\n",
        "import collections  # contains idiomatic data structures\n",
        "import copy\n",
        "import itertools    # provides efficient tools on iterators\n",
        "import re           # regexes\n",
        "\n",
        "from functools import partial  # little helper for partially applying a function\n",
        "from typing import List, Dict, Tuple, Union, NewType, TypeVar, Counter, Iterator  # statically typing for python\n",
        "\n",
        "import matplotlib.pyplot as plt  # plotting tool\n",
        "import nltk                      # natural language processing toolkit\n",
        "import numpy as np               # main scientific linear algebra library in python (matrices)\n",
        "import pandas as pd              # dataframes\n",
        "import sklearn                   # machine learning & data mining library\n",
        "import tqdm                      # progression bar\n",
        "\n",
        "from tensorflow import keras     # high level tensorflow API\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 9)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNr1m0q_Tjo9",
        "colab_type": "text"
      },
      "source": [
        "### Defining some constants and types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WheOgS6TW62r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some usefull types for this dataset\n",
        "\n",
        "InputText = NewType('InputText', Union[str, List[str]])\n",
        "Label = NewType('Label', int)\n",
        "DocumentRecord = NewType('DocumentRecord', Tuple[InputText, Label])\n",
        "Dataset = NewType('Dataset', Dict[str, List[DocumentRecord]])\n",
        "\n",
        "Token = NewType('Token', str)\n",
        "Vocabulary = NewType('Vocabulary', Dict[Token, int])\n",
        "\n",
        "\n",
        "# Constants\n",
        "\n",
        "CLASS_NAMES = ['form', 'email', 'handwritten', 'advertisement', 'invoice']\n",
        "CLASS_INDICES = ['1', '2', '3', '4', '11']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "STOP_WORD_S = set(nltk.corpus.stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD9QVg8KW62v",
        "colab_type": "text"
      },
      "source": [
        "## Load the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeZWsk-AbTc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some local scripts imports\n",
        "import download_dataset  # dowloading from google drive\n",
        "import ocr_input         # deals with reading dataset and xml parsing\n",
        "\n",
        "\n",
        "for elt in ['label', 'ocr', 'dataset_assignment']:\n",
        "  download_dataset.download_and_extract(elt)\n",
        "dataset_path = 'dataset'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngR65gEjW62w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset() -> Dataset:\n",
        "  \"\"\"\n",
        "  Parse all data xml files and make pairs withe their label.\n",
        "  Splits the records into training and test datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  # Parsing xml files into doc_ocr_d\n",
        "  all_files = os.listdir(os.path.join(dataset_path, \"ocr\"))\n",
        "  doc_ocr_d = {file: content for file, content in tqdm.tqdm(zip((os.path.splitext(f)[0] for f in all_files), \n",
        "                                                                (ocr_input.parse_xml(os.path.join(dataset_path, \"ocr\", p)) for p in all_files)),\n",
        "                                                            total=len(all_files))}\n",
        "  \n",
        "  # Fetching labels into label_d\n",
        "  with open(os.path.join(dataset_path, \"label.txt\"), \"r\") as fp:\n",
        "      label_d = {file: CLASS_INDICES.index(label.strip()) for file, label in map(lambda line: line.split(','), fp.readlines())}\n",
        "\n",
        "  # Fetching assignments into dataset_splits\n",
        "  dataset_split = {\"training\": [], \"test\": []}\n",
        "  with open(os.path.join(dataset_path, 'dataset_assignment.txt'), 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      line = line.strip()\n",
        "      file_id, assignment = line.split(',')\n",
        "      dataset_split[assignment].append(file_id)\n",
        "\n",
        "  return {split_name: [(doc_ocr_d[file], label_d[file]) for file in file_split_l] for split_name, file_split_l in dataset_split.items()}\n",
        "\n",
        "dataset = get_dataset()\n",
        "\n",
        "print(f\"Number of training documents: {len(dataset['training'])}\")\n",
        "print(f\"Number of test documents: {len(dataset['test'])}\")\n",
        "\n",
        "x_train, y_train = zip(*dataset[\"training\"])\n",
        "x_test, y_test = zip(*dataset[\"test\"])\n",
        "\n",
        "# Shuffling dataset\n",
        "\n",
        "data_train = list(zip(x_train, y_train))\n",
        "np.random.shuffle(data_train)\n",
        "x_train, y_train = (list(x) for x in zip(*data_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYPlOzpCW622",
        "colab_type": "text"
      },
      "source": [
        "## Study the vocabulary\n",
        "\n",
        "In this part we will look at the data.\n",
        "\n",
        "When dealing with text and words, the first thing to do is looking at those words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW93AiRT8b4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To access a specific element or range in a list, you can use bracket notation: \n",
        "# `my_list[0]` is the first element\n",
        "# `my_list[10: 20]` is an array containing elements from index 10 (included) to 20 (excluded)\n",
        "\n",
        "## Print some texts from the dataset and look at what the OCR system has read. ##\n",
        "\n",
        "\n",
        "## Any remarks ? ##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gha_z0QP0EHJ",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "# Use the function `print` to look at texts in the datasets (either x_train or x_test)\n",
        "# To access a scpecific element or range in a list, you can use bracket notation: \n",
        "# `my_list[0]` is the first element\n",
        "# `my_list[10: 20]` is an array containing elements from index 10 (included) to 20 (excluded)\n",
        "\n",
        "\n",
        "# print some texts from the dataset and look at what the OCR system has read.\n",
        "for i in range(10):\n",
        "  print(i, x_train[np.random.randint(0, len(x_train)-1)])\n",
        "\n",
        "# Any remarks ?\n",
        "\n",
        "\"\"\"\n",
        "Mostly not words, bunch of symbols. Very hard to understand.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmATmRuC-OT4",
        "colab_type": "text"
      },
      "source": [
        "### Raw vocabulary\n",
        "\n",
        "Let's make a vocabulary out of what's in the texts !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAtqeISKW623",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Naive vocabulary counting: splitting on space character\n",
        "\n",
        "# Conventions:\n",
        "# index 0 is reserved for unknown tokens that will be mapped to `__UNK__`.\n",
        "# other special token come just after (eg. `__NUM__` for numbers).\n",
        "# other classic token are inserted in order for reverse dictionnary purpose.\n",
        "__UNK__ = '__UNK__'\n",
        "\n",
        "# always put __UNK__ first when redefining special char.\n",
        "DEFAULT_SPECIALS = [__UNK__]\n",
        "\n",
        "def unknown_wrapped(f):\n",
        "    \"\"\"\n",
        "    A wrapper around a tokenizer that provides a vocabulary parameter.\n",
        "    If vocabulary is not None, generated token are checked against the vocabulary.\n",
        "    If it does not contains this specific token, __UNK__ is yielded instead.\n",
        "    \"\"\"\n",
        "    def wrapped(text, vocabulary=None):\n",
        "        gen = f(text)\n",
        "        if vocabulary is None:\n",
        "            yield from gen\n",
        "        else:\n",
        "            for token in gen:\n",
        "                if token not in vocabulary:\n",
        "                    yield __UNK__\n",
        "                else:\n",
        "                    yield token\n",
        "\n",
        "    return wrapped\n",
        "\n",
        "# the most basic tokenizer: split on space charater\n",
        "@unknown_wrapped\n",
        "def basic_tokenizer(text: str) -> Iterator[Token]:\n",
        "    yield from text.split(\" \")\n",
        "\n",
        "\n",
        "# the most basic preprocess: no preprocess done\n",
        "def no_preprocess(text: str) -> str:\n",
        "    return text\n",
        "\n",
        "\n",
        "def compute_vocabulary(input_text: List[InputText],\n",
        "                       max_size=1000,\n",
        "                       tokenize_f=basic_tokenizer,\n",
        "                       specials=DEFAULT_SPECIALS,\n",
        "                       preprocess_f=no_preprocess) -> Tuple[Vocabulary, Counter[Token]]:\n",
        "    \"\"\"\n",
        "    Given a preprocessing function, a tokenizer and a collection of special tokens,\n",
        "    compute the vocabulary mapping and a corresponding tokenizer and number of occurences of tokens.\n",
        "\n",
        "    main steps:\n",
        "      - First preprocessing is applied to each text.\n",
        "      - Then each preprocessed text is splitted into tokens.\n",
        "      - All tokens from all text are chained together and empty tokens are filtered out.\n",
        "      - Tokens are sorted by reversed number of occurences in the vocabulary.\n",
        "      - A special treatment is reverved for special tokens.\n",
        "\n",
        "    return:\n",
        "      - vocabulary: A mapping from tokens to their corresponding index. Indices start at 0 and end at max_size-1\n",
        "      - word_tokenizer_f: A tokenizer function that only produce tokens included in the vocabulary. (__UNK__ is returned if the token is not in the vocabulary)\n",
        "      - token_occurences_d: A mapping from tokens to their corresponding number of occurences in the texts.\n",
        "    \"\"\"\n",
        "    token_occurences_d = collections.Counter(i for i in itertools.chain(*map(tokenize_f,        # Split into token\n",
        "                                                                            map(preprocess_f,  # Preprocess text before tokenization\n",
        "                                                                                x_train)))\n",
        "                                            if i)                                              # Filter out empty strings\n",
        "\n",
        "\n",
        "    # compute number of missing special tokens in the word occurences\n",
        "    no_missing_special = sum(1 for sp in specials if not sp in token_occurences_d)\n",
        "    vocabulary = collections.OrderedDict([(word, i) for i, (word, _) in enumerate(token_occurences_d.most_common(max_size - no_missing_special), no_missing_special)])\n",
        "\n",
        "    # Put special tokens at the beginning of the vocabulary\n",
        "    i = 1\n",
        "    for sp in reversed(specials):\n",
        "        if sp not in vocabulary:\n",
        "            vocabulary[sp] = no_missing_special - i\n",
        "            vocabulary.move_to_end(sp, last=False)\n",
        "            i += 1\n",
        "\n",
        "    # Specialize the given tokenizer for the computed vocabulary\n",
        "    word_tokenizer_f = partial(tokenize_f, vocabulary=vocabulary)\n",
        "\n",
        "    return vocabulary, word_tokenizer_f, token_occurences_d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n5i6xkXW625",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some plotting functions to display the vocabulary\n",
        "\n",
        "def plot_token_count(token_count, n=1000):\n",
        "    \"\"\"\n",
        "    Plots occurences for n most common tokens\n",
        "    \"\"\"\n",
        "    plt.plot(list(range(n)), [i for _, i in token_count.most_common(n)])\n",
        "\n",
        "    plt.title(f\"Evolution of occurences of the {n} most frequent tokens\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_accumulated_token_count(token_count, n=1000):\n",
        "    \"\"\"\n",
        "    Plots accumulated occurences divided by token number of tokens for n most common tokens \n",
        "    \"\"\"\n",
        "    total_tokens = sum(token_count.values()) / 100\n",
        "    plt.plot(list(range(n)), list(itertools.accumulate(i / total_tokens for _, i in token_count.most_common(n))))\n",
        "\n",
        "    plt.title(f\"Evolution of cumulated occurences of the {n} most frequent tokens divided by total number of tokens\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsIOx-1YIs-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Use the function `compute_vocabulary` to get vocabulary and token_count object. ##\n",
        "\n",
        "## What are the most common tokens ? ##\n",
        "\n",
        "## Plot token occurences and cumulated token occurences. ##\n",
        "\n",
        "## How many percentages of the token are included in the vocabulary if we use 1_000 tokens ? 10_000 ? 100_000 ? ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uiZNl1mW628",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# Use the function `compute_vocabulary` to get vocabulary and token_count object.\n",
        "*_, word_count = compute_vocabulary(x_train, max_size=10**4)\n",
        "\n",
        "\n",
        "# What are the most common tokens\n",
        "print(list(word_count.most_common(100)))\n",
        "\n",
        "# Plot token occurences and cumulated token occurences.\n",
        "plot_token_count(word_count, n=10000)\n",
        "plot_accumulated_token_count(word_count, n=10000)\n",
        "\n",
        "# How many percentages of the token are included in the vocabulary if we use 1_000 tokens ? 10_000 ? 100_000 ?\n",
        "for size in [10**3, 10**4, 10**5]:\n",
        "  print(f\"With a vocabulary of size {size}, you cover {sum(t for _, t in word_count.most_common(size)) / sum(word_count.values()) * 100:0.2f}% of the encountered tokens\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKGIAJU1W62_",
        "colab_type": "text"
      },
      "source": [
        "### We must reduce vocabulary size\n",
        "\n",
        "To clean the texts from all the noise produced by the OCR, we can use advanced preprocessing and tokenizer.\n",
        "\n",
        "The job of the **preprocessing** is to prepare the text to be splitted on space characters. An example of simple preprocessing would be:\n",
        "* Use lowercase only.\n",
        "* Remove useless characters that are unlikely to really be in the document and likely to be noise produced by OCR.\n",
        "* Introduce additional spaces between words and punctuation so \"This is a cat.\" is transformed into \"This is a cat .\" (note the space at the end).\n",
        "\n",
        "The job of the **tokenizer** is to split sentences into separate tokens. Our vocabulary is polluted by multiple punctuation and numbers. A simple workaround is to create special tokens that represent a group of symbols. For example we could introduce a `__NUM__` token wich represent all numbers. Any number in the text would be mapped to `__NUM__`.\n",
        "\n",
        "The resulting vocabulary should include much less noise and a lot a words !\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsDPsGe8P5jV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Implement the preprocessor described above. Feel free to add other steps in the preprocessing. ##\n",
        "\n",
        "def my_preprocess(text: str) -> str:\n",
        "  # Implement here\n",
        "  return text\n",
        "\n",
        "## Implement the tokenizer described above. Examples of groups of tokens are: numbers, punctuation, mix of those... ##\n",
        "\n",
        "# Some categories of character\n",
        "ALPHA = {i for i in \"azertyuiopqsdfghjklmwxcvbn\"}\n",
        "DIGIT = {i for i in \"1234567890\"}\n",
        "PUNCT = {i for i in r\".?,!:$£@/-\\\\\"}\n",
        "\n",
        "\n",
        "__NUM__ = \"__NUM__\"  # Numbers\n",
        "__PUN__ = \"__PUN__\"  # Punctuation\n",
        "__MIX__ = \"__MIX__\"  # Mix of numbers and puntuation\n",
        "MY_SPECIALS = [__UNK__, __NUM__, __PUN__, __MIX__]\n",
        "\n",
        "@unknown_wrapped\n",
        "def my_tokenizer(text: str) -> Iterator[Token]:\n",
        "  for word in text.split(\" \"):\n",
        "    # Implement here, use keyword `yield` instead of return to produce an iterator over your tokens\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT_16-BvW63B",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# Implement the preprocessor described above. Feel free to add other steps in the preprocessing.\n",
        "\n",
        "def regex_preprocess(text: str) -> str:\n",
        "    text = re.sub(r\"(?<=[a-z])([.?,!:])\", r\" \\1\", text.lower())  # Add an extra space around punctuation (usefull in english)\n",
        "    text = re.sub(r\"([.?,!:])(?=[a-z])\", r\"\\1 \", text)           # Add an extra space around punctuation (usefull in english)\n",
        "    return re.sub(r\"[^ a-z0-9.?,!:$£@/\\-\\\\]\", \" \", text)         # Remove any non basic character\n",
        "\n",
        "\n",
        "# Implement the tokenizer described above. Examples of groups of tokens are: numbers, punctuation, mix of those...\n",
        "ALPHA = {i for i in \"azertyuiopqsdfghjklmwxcvbn\"}\n",
        "DIGIT = {i for i in \"1234567890\"}\n",
        "PUNCT = {i for i in r\".?,!:$£@/-\\\\\"}\n",
        "\n",
        "\n",
        "__NUM__ = \"__NUM__\"\n",
        "__PUN__ = \"__PUN__\"\n",
        "__MIX__ = \"__MIX__\"\n",
        "MY_SPECIALS = [__UNK__, __NUM__, __PUN__, __MIX__]\n",
        "\n",
        "\n",
        "@unknown_wrapped\n",
        "def special_tokenizer(text: str) -> Iterator[Token]:\n",
        "    for word in text.split(\" \"):\n",
        "        if not word in STOP_WORD_S:\n",
        "            if all(c in ALPHA for c in word):\n",
        "                yield word\n",
        "            elif all(c in DIGIT for c in word):\n",
        "                yield __NUM__\n",
        "            elif all(c in PUNCT for c in word):\n",
        "                yield __PUN__\n",
        "            else:\n",
        "                yield __MIX__\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1L3v8X_WKvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Same Questions as before but with your new preprocessing and tokenizer ##\n",
        "\n",
        "## What are the most common tokens ##\n",
        "\n",
        "## Plot token occurences and cumulated token occurences. ##\n",
        "\n",
        "## How many percentages of the token are included in the vocabulary if we use 1_000 tokens ? 10_000 ? 100_000 ? ##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ6QaT1et6Z4",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# Same Questions as before but with your new preprocessing and tokenizer\n",
        "*_, word_count = compute_vocabulary(x_train, max_size=10**4, tokenize_f=special_tokenizer, specials=MY_SPECIALS, preprocess_f=regex_preprocess)\n",
        "\n",
        "\n",
        "# What are the most common tokens\n",
        "print(list(word_count.most_common(100)))\n",
        "\n",
        "# Plot token occurences and cumulated token occurences.\n",
        "plot_token_count(word_count, n=10000)\n",
        "plot_accumulated_token_count(word_count, n=10000)\n",
        "\n",
        "# How many percentages of the token are included in the vocabulary if we use 1_000 tokens ? 10_000 ? 100_000 ?\n",
        "for size in [10**3, 10**4, 10**5]:\n",
        "  print(f\"With a vocabulary of size {size}, you cover {sum(t for _, t in word_count.most_common(size)) / sum(word_count.values()) * 100:0.2f}% of the encountered tokens\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFHy4rtVW63D",
        "colab_type": "text"
      },
      "source": [
        "## Basic Model: Bag of Words\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75Bjv_VkSI_g",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizer and Vectorizer\n",
        "\n",
        "To implement a Bag of Word model, we first need to convert sentences to vector using a CountVectorizer.\n",
        "\n",
        "It basically counts how many times each token appears in a text and put each value at each token's index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3sJoJLdW63E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCABULARY_SIZE = 10**5\n",
        "## Use skleanr's CountVectorizer to implement a vectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html ##\n",
        "# Remember to specify the vocabulary, the tokenizer and the preprocessor with your own to erase sklearn's defaults\n",
        "\n",
        "\n",
        "vocabulary, tokenizer_f, _ = compute_vocabulary(x_train, max_size=VOCABULARY_SIZE, tokenize_f=my_tokenizer, preprocess_f=my_preprocess, specials=MY_SPECIALS)\n",
        "# Create the CountVectorizer here\n",
        "\n",
        "\n",
        "bow_x_train, bow_x_test = [x_train, x_test]\n",
        "bow_y_train, bow_y_test = [y_train, y_test]\n",
        "\n",
        "bow_x_train, bow_x_test = map(vectorizer.fit_transform, [bow_x_train, bow_x_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqJIAYkVdXqq",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "VOCABULARY_SIZE = 10**5\n",
        "## Use skleanr's CountVectorizer to implement a vectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html ##\n",
        "\n",
        "# Remember to specify the vocabulary, the tokenizer and the preprocessor with your own to erase sklearn's defaults\n",
        "\n",
        "vocabulary, tokenizer_f, _ = compute_vocabulary(x_train, max_size=VOCABULARY_SIZE, tokenize_f=special_tokenizer, preprocess_f=regex_preprocess, specials=MY_SPECIALS)\n",
        "vectorizer = sklearn.feature_extraction.text.CountVectorizer(vocabulary=vocabulary, tokenizer=tokenizer_f, preprocessor=regex_preprocess, binary=True)\n",
        "\n",
        "\n",
        "bow_x_train, bow_x_test = [x_train, x_test]\n",
        "bow_y_train, bow_y_test = [y_train, y_test]\n",
        "\n",
        "bow_x_train, bow_x_test = map(vectorizer.fit_transform, [bow_x_train, bow_x_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsT6QNB7ezHw",
        "colab_type": "text"
      },
      "source": [
        "The count vectorizer should have its features identicall to our vocabulary.\n",
        "\n",
        "Try to preprocess a text and give it to the count vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW48ask9fcCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Check if your vectorizer is correct with a small sentence ##\n",
        "sentence = \"12 /.1 tobacco is bad.\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttsLscfKW63G",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "## Check if your vectorizer is correct ##\n",
        "\n",
        "sentence = \"12 /.1 tobacco is bad.\"\n",
        "\n",
        "print(vectorizer.get_feature_names() == list(vocabulary.keys()))\n",
        "\n",
        "print(regex_preprocess(sentence))\n",
        "print(vectorizer.fit_transform([sentence]))\n",
        "\n",
        "print(vocabulary[\"tobacco\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpxSVOmIgJE7",
        "colab_type": "text"
      },
      "source": [
        "### Model\n",
        "We will now start building our model.\n",
        "\n",
        "You can use any optimizer (`SGD`, `RMSProp`, …) but `Adam` is one of the best currently. It converges faster and to a better minimum than other optimizers most of the times\n",
        "\n",
        "We are doing a classification problem, use `sparse_categorical_crossentropy` as your loss and `sparse_categorical_accuracy` as your metric.\n",
        "\n",
        "Feel free to try multiple numbers of hidden units, layers, activation functions, add new types of layers (see keras.layers for this: https://keras.io/layers/core/) …\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h59cqjJPkhal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create a Sequential model that takes a sentence vector in input (size=VOCABULARY_SIZE) and returns a vector of size NUM_CLASSES. ##\n",
        "# Find help here: https://keras.io/models/sequential/\n",
        "# and here: https://www.tensorflow.org/tutorials/keras/classification\n",
        "\n",
        "# Create your model here and compile it.\n",
        "model = keras.models.Sequential([\n",
        "\n",
        "])\n",
        "\n",
        "optimizer = None\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iXyfSm0W63J",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "## Create a Sequential model that takes a sentence vector in input (size=VOCABULARY_SIZE) and returns a vector of size NUM_CLASSES. ##\n",
        "# Find help here: https://keras.io/models/sequential/\n",
        "# and here: https://www.tensorflow.org/tutorials/keras/classification\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(input_dim=VOCABULARY_SIZE, units=32, activation=\"relu\"),\n",
        "    keras.layers.Dense(units=32, activation=\"relu\"),\n",
        "    keras.layers.Dense(units=NUM_CLASSES, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCxTv2qDlXyc",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "We are now ready to train our model !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msu69TKbW63L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(bow_x_train, bow_y_train, epochs=15, batch_size=256, validation_split=0.1, shuffle=True, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFJ4aKD_nZ6B",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation\n",
        "We can also evaluate our model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTrjVoYWW63N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(bow_x_test, bow_y_test, verbose=2)\n",
        "\n",
        "bow_y_pred = model.predict_classes(bow_x_test)\n",
        "\n",
        "print(pd.DataFrame(sklearn.metrics.confusion_matrix(bow_y_test, bow_y_pred), columns=CLASS_NAMES, index=CLASS_NAMES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoTPVaKYW63Q",
        "colab_type": "text"
      },
      "source": [
        "## A bit more complex: Recurrent Neural Networks and Long-Short Term Memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQOkCKan1Ia-",
        "colab_type": "text"
      },
      "source": [
        "### Vectorizer\n",
        "A RNN takes as many inputs as tokens in the text. Instead of the previous CountVectorizer, we must implement another vectorizer that return token's index in the vocabulary for each token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2z0k5J1W63R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCABULARY_SIZE = 10**5\n",
        "EMBEDDING_SIZE = 32\n",
        "MAX_SEQ_LEN = 2 * 10**2\n",
        "\n",
        "__PAD__ = \"__PAD__\"\n",
        "PAD_SPECIALS = MY_SPECIALS + [__PAD__]\n",
        "\n",
        "vocabulary, tokenizer_f, _ = compute_vocabulary(x_train, max_size=VOCABULARY_SIZE, tokenize_f=my_tokenizer, preprocess_f=my_preprocess, specials=PAD_SPECIALS)\n",
        "\n",
        "## Implement the vectorizer function that return token's index for each token in each text in the dataset. ##\n",
        "# \n",
        "def build_vectorizer(vocabulary, tokenizer_f, preprocess_f):\n",
        "    def rnn_vectorizer(x: List[InputText]) -> List[List[int]]:\n",
        "      # Implement here  \n",
        "      pass\n",
        "\n",
        "    return rnn_vectorizer\n",
        "\n",
        "vectorizer = build_vectorizer(vocabulary, tokenizer_f, my_preprocess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IMx7NUgUMlB",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "VOCABULARY_SIZE = 10**5\n",
        "EMBEDDING_SIZE = 32\n",
        "MAX_SEQ_LEN = 2 * 10**2\n",
        "\n",
        "__PAD__ = \"__PAD__\"\n",
        "PAD_SPECIALS = MY_SPECIALS + [__PAD__]\n",
        "\n",
        "vocabulary, tokenizer_f, _ = compute_vocabulary(x_train, max_size=VOCABULARY_SIZE, tokenize_f=special_tokenizer, preprocess_f=regex_preprocess, specials=PAD_SPECIALS)\n",
        "\n",
        "## Implement the vectorizer function that return token's index for each token in each text in the dataset. ##\n",
        "# \n",
        "def build_vectorizer(vocabulary, tokenizer_f, preprocess_f):\n",
        "    def rnn_vectorizer(x: List[InputText]):\n",
        "        return [[vocabulary[token] for token in tokenizer_f(preprocess_f(text))] for text in x]\n",
        "    return rnn_vectorizer\n",
        "\n",
        "vectorizer = build_vectorizer(vocabulary, tokenizer_f, regex_preprocess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odwj-QYIVFoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Check the output of your vectorizer with a small sentence ##\n",
        "sentence = \"12 /.1 tobacco is bad.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_di8vi37wItm",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "## Check the output of your vectorizer with a small sentence ##\n",
        "sentence = \"12 /.1 tobacco is bad.\"\n",
        "\n",
        "print(regex_preprocess(sentence))\n",
        "print(vectorizer([sentence]))\n",
        "\n",
        "print(vocabulary[\"tobacco\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxUeoiKsV9vs",
        "colab_type": "text"
      },
      "source": [
        "### Padding\n",
        "\n",
        "All sentences don't contain the same amount of tokens. This means different sentences won't be vectorized to vectors of the same shape. This would make feeding our network very difficult.\n",
        "\n",
        "To overcome this, we pad our vectors to a common shape:\n",
        "* if a vector is smaller, we introduce dummy value at the end to complete the vector.\n",
        "* if a vector is larger, we truncate it's values at the end.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBaJ242hAAK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Use Keras pad_sequence function to implement our padding logic: https://keras.io/preprocessing/sequence/ ##\n",
        "\n",
        "def pad_f(x):\n",
        "  # Implement here using Keras\n",
        "\n",
        "rnn_x_train, rnn_x_test = (pad_f(vectorizer(dataset)) for dataset in [x_train, x_test])\n",
        "rnn_y_train, rnn_y_test = (np.array(labels) for labels in [y_train, y_test])\n",
        "\n",
        "# Shuffle the dataset\n",
        "shuffled_data = list(zip(rnn_x_train, rnn_y_train))\n",
        "np.random.shuffle(shuffled_data)\n",
        "rnn_x_train, rnn_y_train = (np.array(d) for d in zip(*shuffled_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqfPTNK2XE-w",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "## Use Keras pad_sequence function to implement our padding logic: https://keras.io/preprocessing/sequence/ ##\n",
        "\n",
        "pad_f = partial(keras.preprocessing.sequence.pad_sequences, maxlen=MAX_SEQ_LEN, padding=\"post\", truncating=\"post\", value=vocabulary[__PAD__])\n",
        "\n",
        "rnn_x_train, rnn_x_test = (pad_f(vectorizer(dataset)) for dataset in [x_train, x_test])\n",
        "rnn_y_train, rnn_y_test = (np.array(labels) for labels in [y_train, y_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec8JaXaxYl5Y",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnoxz3yBW63T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Like the Bag of Words model, implement a Sequential LSTM model  and compile it##\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length=MAX_SEQ_LEN),  # Embeddings of tokens\n",
        "])\n",
        "\n",
        "optimizer = None\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiIsKmEGYqmD",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "## Like the Bag of Words model, implement a Sequential LSTM model  and compile it##\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length=MAX_SEQ_LEN),  # Embeddings of tokens\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(32)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dense(units=NUM_CLASSES, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnBGJz3-W63V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train your model ##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeZD63n1ZIZL",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "## Train your model ##\n",
        "model.fit(rnn_x_train, rnn_y_train, epochs=15, batch_size=128, validation_split=0.1, verbose=1, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzI58o-TAcHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Evaluate your new model. Is it better than Bag of Words ? CNN ? ##\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAtGigynBOx6",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "## Evaluate your new model. Is it better than Bag of Words ? CNN ? ##\n",
        "\n",
        "model.evaluate(rnn_x_test, rnn_y_test, verbose=2)\n",
        "\n",
        "rnn_pred = model.predict_classes(rnn_x_test)\n",
        "\n",
        "print(pd.DataFrame(sklearn.metrics.confusion_matrix(y_test, rnn_pred), columns=CLASS_NAMES, index=CLASS_NAMES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN3QZJaAbN6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
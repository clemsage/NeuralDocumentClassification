{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clemsage/NeuralDocumentClassification/blob/master/skeleton_ocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "56cA2WtUQZ-I"
      },
      "source": [
        "# Training a classifier on OCR text input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AAd75X5JT4vI"
      },
      "source": [
        "## Imports & Cloning repository\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uefjUmcMRnLT"
      },
      "source": [
        "### Import Tensorflow v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mVIlDgNHW62j"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_text\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ak-qq01rXLxL"
      },
      "outputs": [],
      "source": [
        "# Check your devices, if it fails change your execution context to GPU\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B7Ki-ZY9RyXl"
      },
      "source": [
        "### Usefull imports and clone repo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3vFz745yYfbc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HL_khQsbYY7O"
      },
      "outputs": [],
      "source": [
        "# Clone the git repository\n",
        "\n",
        "if not os.path.exists('NeuralDocumentClassification'):\n",
        "  !git clone https://github.com/clemsage/NeuralDocumentClassification.git\n",
        "else:\n",
        "  !git -C NeuralDocumentClassification pull\n",
        "sys.path.append('NeuralDocumentClassification')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lp4eXRIKW62o"
      },
      "outputs": [],
      "source": [
        "# Lot of usefull imports\n",
        "\n",
        "# All of them are already installed on the colab session\n",
        "\n",
        "\n",
        "# STD imports\n",
        "import collections  # contains idiomatic data structures\n",
        "import copy\n",
        "import itertools    # provides efficient tools on iterators\n",
        "import random\n",
        "import re           # regexes\n",
        "\n",
        "from functools import partial  # little helper for partially applying a function\n",
        "from typing import List, Dict, Tuple, Union, NewType, TypeVar, Counter, Iterator, Callable  # statically typing for python\n",
        "\n",
        "import matplotlib.pyplot as plt  # plotting tool\n",
        "import nltk                      # natural language processing toolkit\n",
        "import numpy as np               # main scientific linear algebra library in python (matrices)\n",
        "import pandas as pd              # dataframes\n",
        "import sklearn                   # machine learning & data mining library\n",
        "import tensorflow as tf          # machine learning library\n",
        "import tensorflow_text as tf_text# bonus text utilities for tensorflow\n",
        "import tqdm                      # progression bar\n",
        "\n",
        "\n",
        "from tensorflow import keras     # high level tensorflow API\n",
        "from sklearn import metrics      # metrics for model performances\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KNr1m0q_Tjo9"
      },
      "source": [
        "### Defining some constants and types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WheOgS6TW62r"
      },
      "outputs": [],
      "source": [
        "# Some usefull types for this dataset\n",
        "\n",
        "InputText = NewType('InputText', Union[str, List[str]])\n",
        "Label = NewType('Label', int)\n",
        "DocumentRecord = NewType('DocumentRecord', Tuple[InputText, Label])\n",
        "Dataset = NewType('Dataset', Dict[str, List[DocumentRecord]])\n",
        "\n",
        "Token = NewType('Token', str)\n",
        "Vocabulary = NewType('Vocabulary', Dict[Token, int])\n",
        "\n",
        "\n",
        "# Constants\n",
        "\n",
        "CLASS_NAMES = ['form', 'email', 'handwritten', 'advertisement', 'invoice']\n",
        "CLASS_INDICES = ['1', '2', '3', '4', '11']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YD9QVg8KW62v"
      },
      "source": [
        "## Load the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yeZWsk-AbTc_"
      },
      "outputs": [],
      "source": [
        "# Some local scripts imports\n",
        "import download_dataset  # dowloading from google drive\n",
        "import ocr_input         # deals with reading dataset and xml parsing\n",
        "\n",
        "\n",
        "for elt in ['label', 'ocr', 'dataset_assignment']:\n",
        "  download_dataset.download_and_extract(elt)\n",
        "dataset_path = 'dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ngR65gEjW62w"
      },
      "outputs": [],
      "source": [
        "def get_dataset(clean_text_f: Callable = (lambda x: x)):\n",
        "  labels = {}\n",
        "  with open(os.path.join(dataset_path, \"label.txt\"), \"r\") as f:\n",
        "      for line in f.readlines():\n",
        "          file_id, lbl = line.strip().split(\",\")\n",
        "          labels[file_id] = lbl\n",
        "\n",
        "  dataset = collections.defaultdict(list)\n",
        "  with open(os.path.join(dataset_path, \"dataset_assignment.txt\"), \"r\") as f:\n",
        "      for line in f.readlines():\n",
        "          line = line.split('\\n')[0]\n",
        "          file_id, assignment = line.split(',')\n",
        "          img_path = os.path.join(dataset_path, \"image_png\", f\"{file_id}.png\")\n",
        "          ocr_path = os.path.join(dataset_path, \"ocr\", f\"{file_id}.xml\")\n",
        "          \n",
        "          text = ocr_input.parse_xml(ocr_path)\n",
        "          text = clean_text_f(text) \n",
        "          \n",
        "          dataset[f\"{assignment}_ocr\"].append(text)\n",
        "          dataset[f\"{assignment}_lbl\"].append(CLASS_INDICES.index(labels[file_id]))\n",
        "  return dataset\n",
        "\n",
        "dataset = get_dataset()\n",
        "\n",
        "print(f\"Number of training documents: {len(dataset['training_ocr'])}\")\n",
        "print(f\"Number of test documents: {len(dataset['test_ocr'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lYPlOzpCW622"
      },
      "source": [
        "## Study the vocabulary\n",
        "\n",
        "In this part we will look at the data.\n",
        "\n",
        "When dealing with text and words, the first thing to do is looking at those words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iW93AiRT8b4d"
      },
      "outputs": [],
      "source": [
        "# To access a specific element or range in a list, you can use bracket notation: \n",
        "# `my_list[0]` is the first element\n",
        "# `my_list[10: 20]` is an array containing elements from index 10 (included) to 20 (excluded)\n",
        "\n",
        "## Print some texts from the dataset and look at what the OCR system has read. ##\n",
        "\n",
        "\n",
        "## Any remarks ? ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "gha_z0QP0EHJ"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "# Use the function `print` to look at texts in the datasets (either x_train or x_test)\n",
        "# To access a scpecific element or range in a list, you can use bracket notation: \n",
        "# `my_list[0]` is the first element\n",
        "# `my_list[10: 20]` is an array containing elements from index 10 (included) to 20 (excluded)\n",
        "\n",
        "\n",
        "# print some texts from the dataset and look at what the OCR system has read.\n",
        "for x in random.choices(dataset[\"training_ocr\"], k=5):\n",
        "  print(x)\n",
        "\n",
        "\n",
        "# Any remarks ?\n",
        "\n",
        "\"\"\"\n",
        "Mostly not words, bunch of symbols. Very hard to understand.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vmATmRuC-OT4"
      },
      "source": [
        "### Vocabulary\n",
        "\n",
        "Let's find out of what's in the texts and clean it a bit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0n5i6xkXW625"
      },
      "outputs": [],
      "source": [
        "# Some plotting functions to display the vocabulary\n",
        "\n",
        "def plot_token_count(token_count, n=1000):\n",
        "    \"\"\"\n",
        "    Plots occurences for n most common tokens\n",
        "    \"\"\"\n",
        "    plt.plot(list(range(n)), [i for _, i in token_count.most_common(n)])\n",
        "\n",
        "    plt.yscale(\"log\")\n",
        "    plt.title(f\"Evolution of occurences of the {n} most frequent tokens\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_accumulated_token_count(token_count, n=1000):\n",
        "    \"\"\"\n",
        "    Plots accumulated occurences divided by token number of tokens for n most common tokens \n",
        "    \"\"\"\n",
        "    total_tokens = sum(token_count.values()) / 100\n",
        "    plt.plot(list(range(n)), list(itertools.accumulate(i / total_tokens for _, i in token_count.most_common(n))))\n",
        "\n",
        "    plt.title(f\"Evolution of cumulated occurences of the {n} most frequent tokens divided by total number of tokens\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NsIOx-1YIs-3"
      },
      "outputs": [],
      "source": [
        "## Use the `collections.Counter` class to count each word occurence\n",
        "\n",
        "## What are the most common tokens ? ##\n",
        "\n",
        "## Plot token occurences and cumulated token occurences. ##\n",
        "\n",
        "## How many percentages of the token are included in the vocabulary if we use 1_000 tokens ? 10_000 ? 100_000 ? ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "6uiZNl1mW628"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Count each word occurence\n",
        "word_count = Counter()\n",
        "for text in dataset[\"training_ocr\"]:\n",
        "  word_count.update(text.split())\n",
        "\n",
        "\n",
        "# What are the most common tokens\n",
        "print(list(word_count.most_common(100)))\n",
        "\n",
        "# Plot token occurences and cumulated token occurences.\n",
        "plot_token_count(word_count, n=10000)\n",
        "plot_accumulated_token_count(word_count, n=10000)\n",
        "\n",
        "# How many percentages of the token are included in the vocabulary if we use 1_000 tokens ? 10_000 ? 100_000 ?\n",
        "for size in [10**3, 10**4, 10**5]:\n",
        "  print(f\"With a vocabulary of size {size}, you cover {sum(t for _, t in word_count.most_common(size)) / sum(word_count.values()) * 100:0.2f}% of the encountered tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you want, you can clean up the texts according to your observations by removing stop words or special characters\n",
        "# Define a function that takes one document text and cleans it and provide it to `get_dataset` function\n",
        "\n",
        "STOP_WORD_S = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "\n",
        "# Example for removing stopwords\n",
        "def clean_text(text):\n",
        "    words = text.lower().split()\n",
        "    return \" \".join([w for w in words if w not in STOP_WORD_S])\n",
        "\n",
        "dataset = get_dataset(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now we can build the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VOCABULARY_SIZE = 10**4\n",
        "\n",
        "tokenizer = tf_text.WhitespaceTokenizer()\n",
        "\n",
        "# Takes text and label, returns tokenized text and label\n",
        "def process_text(text, lbl):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Assign same random index to each word by hashing it\n",
        "    tokens = tf.strings.to_hash_bucket_fast(tokens, VOCABULARY_SIZE)\n",
        "    return tokens[:400], lbl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_txt_ds = tf.data.Dataset.from_tensor_slices((dataset[\"training_ocr\"], dataset[\"training_lbl\"]))\n",
        "train_txt_ds = train_txt_ds.shuffle(100000)\n",
        "train_txt_ds = train_txt_ds.map(process_text)\n",
        "\n",
        "test_txt_ds = tf.data.Dataset.from_tensor_slices((dataset[\"test_ocr\"], dataset[\"test_lbl\"]))\n",
        "test_txt_ds = test_txt_ds.map(process_text)\n",
        "\n",
        "print(next(iter(train_txt_ds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FFHy4rtVW63D"
      },
      "source": [
        "## Basic Model: Bag of Words\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "75Bjv_VkSI_g"
      },
      "source": [
        "### Vectorizer\n",
        "\n",
        "To implement a Bag of Word model, we first need to represent sentences according to bag of words\n",
        "Each document must be represented by a `VOCABULARY_SIZE` length vector counting occurences of each word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e3sJoJLdW63E"
      },
      "outputs": [],
      "source": [
        "# use `tf.math.bincount` to count word occurences.\n",
        "# Can you find how to implement binary bag of words ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "tqJIAYkVdXqq"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "def convert_to_bow(tokens, lbl):\n",
        "    # Use param `binary_output=True` for binary counting\n",
        "    emb = tf.math.bincount(tf.cast(tokens, tf.int32), minlength=VOCABULARY_SIZE)\n",
        "    return emb, lbl\n",
        "\n",
        "train_txt_bow_ds = train_txt_ds.map(convert_to_bow)\n",
        "test_txt_bow_ds = test_txt_ds.map(convert_to_bow)\n",
        "\n",
        "print(next(iter(train_txt_bow_ds)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JpxSVOmIgJE7"
      },
      "source": [
        "### Model\n",
        "We will now start building our model.\n",
        "\n",
        "You can use any optimizer (`SGD`, `RMSProp`, …) but `Adam` is one of the best currently. It converges faster and to a better minimum than other optimizers most of the times\n",
        "\n",
        "We are doing a classification problem, use `sparse_categorical_crossentropy` as your loss and `sparse_categorical_accuracy` as your metric.\n",
        "\n",
        "Feel free to try multiple numbers of hidden units, layers, activation functions, add new types of layers (see keras.layers for this: https://keras.io/layers/core/) …\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "h59cqjJPkhal"
      },
      "outputs": [],
      "source": [
        "## Create a Sequential model that takes a sentence vector in input (size=VOCABULARY_SIZE) and returns a vector of size NUM_CLASSES. ##\n",
        "# Find help here: https://keras.io/models/sequential/\n",
        "# and here: https://www.tensorflow.org/tutorials/keras/classification\n",
        "\n",
        "# Create your model here and compile it.\n",
        "model = keras.models.Sequential([\n",
        "\n",
        "])\n",
        "\n",
        "# Select and optimizer\n",
        "optimizer = None\n",
        "\n",
        "# Compile model with loss, optimizer and metrics\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "9iXyfSm0W63J"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "## Create a Sequential model that takes a sentence vector in input (size=VOCABULARY_SIZE) and returns a vector of size NUM_CLASSES. ##\n",
        "# Find help here: https://keras.io/models/sequential/\n",
        "# and here: https://www.tensorflow.org/tutorials/keras/classification\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(input_dim=VOCABULARY_SIZE, units=32, activation=\"relu\"),\n",
        "    keras.layers.Dense(units=32, activation=\"relu\"),\n",
        "    keras.layers.Dense(units=NUM_CLASSES, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cCxTv2qDlXyc"
      },
      "source": [
        "### Training\n",
        "We are now ready to train our model !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Msu69TKbW63L"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "model.fit(train_txt_bow_ds.batch(batch_size),\n",
        "          epochs=15,\n",
        "          validation_data=test_txt_bow_ds.batch(8),\n",
        "          callbacks=[tf.keras.callbacks.TensorBoard(\"logs/nlp\")],\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FFJ4aKD_nZ6B"
      },
      "source": [
        "### Evaluation\n",
        "We can also evaluate our model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bTrjVoYWW63N"
      },
      "outputs": [],
      "source": [
        "loss, metric = model.evaluate(test_txt_bow_ds.batch(8), verbose=2)\n",
        "\n",
        "bow_y_pred = model.predict(test_txt_bow_ds.batch(8)).argmax(-1)\n",
        "bow_y_test = [lbl for _, lbl in test_txt_bow_ds.as_numpy_iterator()]\n",
        "\n",
        "print(pd.DataFrame(metrics.confusion_matrix(bow_y_test, bow_y_pred), columns=CLASS_NAMES, index=CLASS_NAMES))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QoTPVaKYW63Q"
      },
      "source": [
        "## A bit more complex: Recurrent Neural Networks and Long-Short Term Memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ec8JaXaxYl5Y"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Fnoxz3yBW63T"
      },
      "outputs": [],
      "source": [
        "## Like the Bag of Words model, implement a Sequential LSTM model  and compile it\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    tf.keras.Input(shape=(None,), dtype=tf.int64, ragged=True),\n",
        "    keras.layers.Embedding(input_dim=VOCABULARY_SIZE, output_dim=EMBEDDING_SIZE)\n",
        "])\n",
        "\n",
        "optimizer = None\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "hiIsKmEGYqmD"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "## Like the Bag of Words model, implement a Sequential LSTM model  and compile it##\n",
        "\n",
        "EMBEDDING_SIZE = 128\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    tf.keras.Input(shape=(None,), dtype=tf.int64, ragged=True),\n",
        "    keras.layers.Embedding(input_dim=VOCABULARY_SIZE, output_dim=EMBEDDING_SIZE),  # Embeddings of tokens\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(32)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dense(units=NUM_CLASSES, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DnBGJz3-W63V"
      },
      "outputs": [],
      "source": [
        "## Train your model ##\n",
        "# Use `padded_batch` instead of `batch` because documents are not all the same length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "CeZD63n1ZIZL"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "## Train your model ##\n",
        "batch_size = 128\n",
        "\n",
        "model.fit(train_txt_ds.padded_batch(batch_size),\n",
        "          epochs=15,\n",
        "          validation_data=test_txt_ds.padded_batch(8),\n",
        "          callbacks=[tf.keras.callbacks.TensorBoard(\"logs/nlp\")],\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VzI58o-TAcHA"
      },
      "outputs": [],
      "source": [
        "## Evaluate your new model. Is it better than Bag of Words ? CNN ? ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "jAtGigynBOx6"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "## Evaluate your new model. Is it better than Bag of Words ? CNN ? ##\n",
        "\n",
        "loss, metric = model.evaluate(test_txt_ds.padded_batch(8), verbose=2)\n",
        "\n",
        "rnn_y_pred = model.predict(test_txt_ds.padded_batch(8)).argmax(-1)\n",
        "rnn_y_test = [lbl for _, lbl in test_txt_ds.as_numpy_iterator()]\n",
        "\n",
        "print(pd.DataFrame(metrics.confusion_matrix(rnn_y_test, rnn_y_pred), columns=CLASS_NAMES, index=CLASS_NAMES))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# If you have finished\n",
        "Take a look at HuggingFace transformers library and try using BERT model: https://huggingface.co/docs/transformers/model_doc/bert\n",
        "You should use TF BERT Tokenizer instead of the Whitespace tokenizer: https://www.tensorflow.org/text/api_docs/python/text/BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YN3QZJaAbN6-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "skeleton_ocr.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "orig_nbformat": 2,
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
